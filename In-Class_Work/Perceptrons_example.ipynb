{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perceptrons.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "S60mYtBpxR61",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, we need to import some of the libraries we will need, as usual. "
      ]
    },
    {
      "metadata": {
        "id": "IrQCRcQOV0w2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os, sys\n",
        "import numpy as np\n",
        "from scipy.misc import bytescale, imresize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "__MSu5coxa3p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we tell python to try and import the progressbar module, but if it receives an error trying to import it, install it first. "
      ]
    },
    {
      "metadata": {
        "id": "5tOs6VZGzBl6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Try whatever it says in the indented line below\n",
        "try:\n",
        "  import progressbar  # import a tool called progressbar\n",
        "  \n",
        "# if you get this specific error (ImportError), do the things in the indented lines below  \n",
        "except ImportError:\n",
        "  !pip3 install -q progressbar2   # install progressbar first\n",
        "  import progressbar  # then import it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qox7N-Zlx5QL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we use the *try...except* approach again to see if the notebook has a link to our Google Drive or not, and if it doesn't, it will install the necessary tools and execute the code to link them. Don't worry about this block, it will be the same thing every time...just need to copy and past it."
      ]
    },
    {
      "metadata": {
        "id": "oJ8TqLPSReEN",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "opzwhOHlZUgx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we need to navigate to our DeepLearningFall2018 folder in our drive. "
      ]
    },
    {
      "metadata": {
        "id": "bc6q19ofYunB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(os.getcwd()) # find out where we are in the drive\n",
        "print(os.listdir())  # find out what is in the current folder in our drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rtJ4jyo7Zlzt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('drive')  # enter the folder called drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I8w8gRXHcADq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('DeepLearningFall2018 (f9fa8bfb)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yp3zpJbyyTgU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For this first exercise on perceptrons, we will need to open a file called **linear_data.csv** by using numpy's *genfromtxt* method. You can open up the file in Google Sheets and see what is actually in this file. It contains 1000 rows (data samples) and 3 attributes for each sample (columns). The first and second attributes are the sample's x- and y-coordinates, and the third attribute is either a 1 or a 0, which determines what class that sample is in."
      ]
    },
    {
      "metadata": {
        "id": "zrJCH3MFWCnL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# read in the data file called linear_data.csv\n",
        "data = np.genfromtxt('linear_data.csv', delimiter=',')\n",
        "print(data.shape)  # print the shape of the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rsY8MnUvyh8U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, let's separate the labels from the data. Since the labels are in the last column of the variable **data**, we can say we want every row --- indicated by the colon --- and just the last column of **data** --- indicated by the -1. Then, we reassign the variable **data** to every row and the first two columns. "
      ]
    },
    {
      "metadata": {
        "id": "YRFqkoFhWXkc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels = data[:, -1]  # take the labels from the data...labels were last column\n",
        "data = data[:, :-1]  # now remove the last column from the data...bc that was the labels\n",
        "print(data.shape)  # print data shape\n",
        "print(labels.shape) # print labels shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MY01yQTC2Jib",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's plot the data to see what it looks like. First, we plot all of the samples in the 0 class as red dots, and then we do the same for all of the samples in class 1 except for with blue dots. It looks like these two classes are able to be separated with a straight line, so a single perceptron will be just fine. "
      ]
    },
    {
      "metadata": {
        "id": "OV6VIY7uYSXz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# make a scatter plot to visualize the data\n",
        "plt.scatter(data[labels == 1, 0], data[labels == 1, 1], c='r')\n",
        "plt.scatter(data[labels == 0, 0], data[labels == 0, 1], c='b')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h6dtJicO7SrO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is the general structure of a perceptron. It takes some inputs, weights each one differently, sums them, and produces an ouput that determines the category these inputs belong to."
      ]
    },
    {
      "metadata": {
        "id": "9MLTJau_rPkQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# read in the file perceptron_image.png as an image...\n",
        "# make sure its in the folder python is looking in\n",
        "perceptron_img = plt.imread('perceptron_image.png')\n",
        "\n",
        "\n",
        "plt.imshow(perceptron_img)  # create a figure and show the image \n",
        "plt.grid(False)  # get rid of the grid lines because this is an image\n",
        "plt.show()  # show the figure"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XA5wxHeo5Eoc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we need to add a column of ones to the end of the data. This is done because the solution to separate these two classes is a straight line, and this column essentially acts as the y-intercept. The input will always be 1 for this last column but how much it is weighted will change, which will change the intercept. "
      ]
    },
    {
      "metadata": {
        "id": "wrJ80K7P5Drw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create biases for the network\n",
        "biases = np.ones([data.shape[0], 1])\n",
        "\n",
        "# add the column of ones to the dataset at the end so each sample also has a \n",
        "# bias of value 1 as third input\n",
        "data = np.concatenate((data, biases), 1)\n",
        "\n",
        "# print the shape of data to make sure it worked correctly\n",
        "print(data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ayRJJcy7Dsl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now need to make a 3-dimensional vector which contains the weights for each of the three aspects of the input (x-coordinate, y-coordinate, and y-intercept). These are the values that will be adjusted during training as the perceptron figures out how to weight each of the inputs so it can predict the sample's class correctly. "
      ]
    },
    {
      "metadata": {
        "id": "jLHsmuGVZUJz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# weight matrix = [input dim + 1, number of perceptrons]\n",
        "# initialize random weights. 3 weights, two for each data input and one for\n",
        "# the bias\n",
        "weights = np.random.randn(3, )\n",
        "print(weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dT4POLJL9QjV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We set the learning rate to 0.05. This number represents how much the weights will change depending on the current samples the perceptron is adjusting to. We don't want this too low, because it will take too long to learn. We also don't want it too high because it will adjust too much to the sample it is currently learning on which could cause it to overfit to that specific sample and cause the learning to be erratic. "
      ]
    },
    {
      "metadata": {
        "id": "PPb0uwDwlJmW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lr = 0.05  # how much to change the weight values each time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nULRQHcg-Oj8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below, we initialize an empty list where we will add each training iteration's error to for later visualization. "
      ]
    },
    {
      "metadata": {
        "id": "WVPCoJSI-NuS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "se = []  # create an empty list called se"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cYapAFJi-V7w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below, we initialize the progressbar object which is a cool and useful way to monitor iterative algorithms / for loops. "
      ]
    },
    {
      "metadata": {
        "id": "EaJuuYVEzLMs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bar = progressbar.ProgressBar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T7DuALY--jw7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is the main training loop. We are going to have the perceptron look at one sample at a time and predict what class it thinks it is in based on its current weights. It will take each input and multiply by its respective weight, then sum all three products up. We can do this easily with the *np.dot* function. Next, it will compare its output with the label and add that value to the stored errors to be plotted later. To update the weights, we first multiply the output error by each input value, which is a crude way of finding out how much each specific input contributed to the output. Finally, we adjust the weights a little bit based on this error and the value of the learning rate."
      ]
    },
    {
      "metadata": {
        "id": "ssed8uWKcHoO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# have i count up to the number of samples in the data and each time, do what \n",
        "# the indented lines below say\n",
        "for i in bar(range(data.shape[0])):\n",
        "  x = data[i, :]  # assign x to the sample represented by i\n",
        "  y = labels[i]  # assign y to the label represented by i\n",
        "  \n",
        "  # perform the dot product between inputs and their respective weights\n",
        "  out = np.dot(x, weights)\n",
        "  \n",
        "  # perform the activation function on the output...in this case output a 1 if \n",
        "  # the number is greater or equal to 0.5 or a 0 if the number is less than 0.5.\n",
        "  out = np.round(out)\n",
        "  \n",
        "  # get the error between the actual output and the desired output (label)\n",
        "  error = y - out\n",
        "  \n",
        "  # add the error for that sample to the list called se so we can plot how the \n",
        "  # error changed during training later\n",
        "  se.append(error)\n",
        "  \n",
        "  # update the weights by multiplying the error by each input value and scaling\n",
        "  # by the learning rate\n",
        "  weights += lr * (error * x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C4pJ6Ntc9rwh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here, we only had the perceptron look at each sample once, but it would easily be possible to modify the training loop to have it go through the dataset multiple times. Below, we can send the entire dataset through and see how many of the training samples it can correctly predict the class of."
      ]
    },
    {
      "metadata": {
        "id": "ZirTdRNkcdpN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# send all of the data through and get the output for every sample\n",
        "all_out = np.round(np.matmul(data, weights))\n",
        "\n",
        "# calculate the accuracy over the training samples by seeing if the output \n",
        "# for each sample matches the label for that sample. If it does, it will return\n",
        "# 1, if not it will return 0 for that sample. Then take the mean to get mean accuracy. \n",
        "acc = np.mean(all_out == labels)\n",
        "print(acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OBgG09ud_PUo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can also see what the new values of the weights are and how each input attribute was weighted. "
      ]
    },
    {
      "metadata": {
        "id": "7gtYi_YBn103",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k_GBHINr_Yc2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we want to separate the training data into two classes based on the perceptron's output to visualize how they were divided."
      ]
    },
    {
      "metadata": {
        "id": "D21IBbwduYph",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " # get all of the data samples where the perceptron pridicted 0\n",
        "pred_neg = data[all_out == 0, :]\n",
        "\n",
        "# get all of the data samples where the perceptron predicted 1\n",
        "pred_pos = data[all_out == 1, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Be2Ea-u_881",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can also pick out which of the predictions it got wrong to see where these points lie. "
      ]
    },
    {
      "metadata": {
        "id": "oiin-F1avUid",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# calculate the difference between each label and its respective ouptut\n",
        "diff = labels - all_out\n",
        "\n",
        "# take the samples out of data where the difference is not 0 (the ones it got wrong.)\n",
        "wrong_pred = data[diff != 0, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hHHsV4i2AH1i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below, we plot the training error over the 1000 iterations, the prediction of each point and where it lies, and the ones it got wrong, each in a different subplot. "
      ]
    },
    {
      "metadata": {
        "id": "ryJtM58po4Ec",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a figure\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "# create three subplots within the figure in a row\n",
        "subplot1 = fig.add_subplot(131)\n",
        "subplot2 = fig.add_subplot(132)\n",
        "subplot3 = fig.add_subplot(133)\n",
        "\n",
        "# set the xlabels and ylabels for subplot 1\n",
        "subplot1.set_xlabel('training iteration')\n",
        "subplot1.set_ylabel('Error Squared')\n",
        "\n",
        "# tell subplot 1 what it's supposed to show\n",
        "subplot1.plot(np.absolute(se))\n",
        "\n",
        "# set just a title for subplot 2\n",
        "subplot2.set_title('Classification based on learned weights')\n",
        "\n",
        "# tell subplot 2 to make a scatter plot where the ones predicted to be\n",
        "# 1 by the perceptron are colored red and their x and y locations\n",
        "subplot2.scatter(pred_pos[:, 0], pred_pos[:, 1], c='r')\n",
        "\n",
        "# tell subplot 2 to also make the negative predictions blue and plot them\n",
        "# according to their x and y values\n",
        "subplot2.scatter(pred_neg[:, 0], pred_neg[:, 1], c='b')\n",
        "\n",
        "# make a title for the third subplot\n",
        "subplot3.set_title('Incorrectly Classified Points')\n",
        "\n",
        "# make a scatter plot using the ones it got wrong\n",
        "subplot3.scatter(wrong_pred[:, 0], wrong_pred[:, 1], c='g')\n",
        "\n",
        "# use this when there are a lot of subplots/words...sometimes doesn't show right otherwise\n",
        "plt.tight_layout()\n",
        "plt.show() # show the plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qrhzlpiXBep6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's create some new data and send that through to see how the perceptron does on data it didn't use for training. "
      ]
    },
    {
      "metadata": {
        "id": "ES96U1U3BpEF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create 1000 more random samples\n",
        "test_data = np.random.rand(1000, 2)\n",
        "\n",
        "# add biases to these samples as the third column\n",
        "test_data = np.concatenate((test_data, biases), 1)\n",
        "\n",
        "# get the test predictions\n",
        "test_out = np.round(np.matmul(test_data, weights))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kjUycpjXCMP1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# once again, separate the data into positively predicted samples\n",
        "test_pos = test_data[test_out == 1, :]\n",
        "\n",
        "# and negatively predicted ones\n",
        "test_neg = test_data[test_out == 0, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IOWGOE8nCSFK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# make a plot of the predicted outputs of this test dataset\n",
        "# to see how well it did \n",
        "plt.scatter(test_pos[:, 0], test_pos[:, 1], c='r')\n",
        "plt.scatter(test_neg[:, 0], test_neg[:, 1], c='b')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gLMYNKl0DvfK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Try it on your own: \n",
        "On your own, rerun the previous steps and retrain a perceptron but without a bias weight to see how the perceptron's solution and accuracy changes without the bias."
      ]
    },
    {
      "metadata": {
        "id": "642RS0Jp1aQl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Multilayer Perceptrons"
      ]
    },
    {
      "metadata": {
        "id": "Zj5EKUvhE9-N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we will use a multilayer perceptron to categorize images of handwritten numbers into different categories based on the number. First, make sure TFLearn --- library that makes Tensorflow code easier to understand and more succinct --- is imported. "
      ]
    },
    {
      "metadata": {
        "id": "iZsxz-yGu2VL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import tflearn\n",
        "except ImportError:\n",
        "  !pip3 install tflearn\n",
        "  import tflearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cpvuPTl-xXCY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We also need to import TensorFlow itself, as well as a couple others."
      ]
    },
    {
      "metadata": {
        "id": "IPvdYVzCqBJU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib.tensorboard.plugins import projector\n",
        "import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NmDo7I-DFNg9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TFLearn has this dataset of handwritten digits (called MNIST) built in, so it is possible to load the data as follows. It is separated into 55000 images for training and 10000 images to test on, which each have 784 pixels (28 x 28). "
      ]
    },
    {
      "metadata": {
        "id": "UNNZuLOI1k0q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tflearn.datasets.mnist as mnist\n",
        "X, Y, testX, testY = mnist.load_data(one_hot=True)\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print(testX.shape)\n",
        "print(testY.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QcyIOvVeQClw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each image has a white/gray number with a black background. For visualization it is easier if the back ground was white and the number was black, so we can subtract each pixel value from 1 to easily accomplish this. "
      ]
    },
    {
      "metadata": {
        "id": "1aDOvJTDnUFC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X, testX = 1. - X, 1. - testX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Le7cweYZF80F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below, we make a simple function which will take in any amount of these images, turn them back into pictures, and then show them all at once. A function like this, which is becoming built-in to many libraries, is very useful for exploring and visualizing data."
      ]
    },
    {
      "metadata": {
        "id": "pkn9ZliP3KXx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def montage(x, return_grid=False):\n",
        "  num = int(np.sqrt(x.shape[0]))\n",
        "  m = int(np.ceil(np.sqrt(x.shape[1])))\n",
        "  n = m\n",
        "  grid = np.zeros([num*m, num*n])\n",
        "  \n",
        "  for i in range(num):\n",
        "    for j in range(num):\n",
        "      grid[i*m:i*m+m, j*n:j*n+n] = bytescale(x[i*num+j, ...].reshape([28, 28]))\n",
        "      \n",
        "  if return_grid:\n",
        "    return grid\n",
        "      \n",
        "  fig = plt.figure(figsize=(15, 15))\n",
        "  a1 = fig.add_subplot(111)\n",
        "  a1.imshow(grid, cmap='gray')\n",
        "  a1.grid(False)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KpPMhPRAb7cR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The two functions below this are there to start Tensorboard during training. "
      ]
    },
    {
      "metadata": {
        "id": "roKxcvZGaxsV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def install_tensorboard_dep():\n",
        "  if 'ngrok-stable-linux-amd64.zip' not in os.listdir(os.getcwd()):\n",
        "    !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "    !unzip ngrok-stable-linux-amd64.zip\n",
        "    os.system('n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y0uSagmL9mcI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def start_tensorboard():\n",
        "  LOG_DIR = '/tmp/tflearn_logs'\n",
        "  get_ipython().system_raw('tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'.format(LOG_DIR))\n",
        "  get_ipython().system_raw('./ngrok http 6006 &')\n",
        "  ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "  \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jhvp39HRQhhf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below, we describe a function that will allow us to visualize the image space later in TensorBoard. "
      ]
    },
    {
      "metadata": {
        "id": "kA7LI9Nl3OUj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def viz_mnist_embedding(tensor, images, labels):\n",
        "  \n",
        "  tb_dir = '/tmp/tflearn_logs'\n",
        "  sess = tf.Session()\n",
        "  sess.run(tensor.initializer)\n",
        "  summary_writer = tf.summary.FileWriter(tb_dir)\n",
        "  config = projector.ProjectorConfig()\n",
        "  embedding = config.embeddings.add()\n",
        "  embedding.tensor_name = tensor.name\n",
        "  embedding.metadata_path = os.path.join(tb_dir, 'metadata.tsv')\n",
        "  embedding.sprite.image_path = os.path.join(tb_dir, 'mnistdigits.png') \n",
        "  embedding.sprite.single_image_dim.extend([28,28])\n",
        "  projector.visualize_embeddings(summary_writer, config)\n",
        "  saver = tf.train.Saver([tensor])\n",
        "  saver.save(sess, os.path.join(tb_dir, 'mnist_fc.ckpt'), 1)\n",
        "  \n",
        "  image_grid = montage(images, True)\n",
        "  plt.imsave(os.path.join(tb_dir, 'mnistdigits.png'), image_grid, cmap='gray')\n",
        "  \n",
        "  with open(os.path.join(tb_dir, 'metadata.tsv'),'w') as f:\n",
        "    f.write(\"Index\\tLabel\\n\")\n",
        "    for index,label in enumerate(labels):\n",
        "      f.write(\"%d\\t%d\\n\" % (index,label))\n",
        "  f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yhH5wqEtGcpC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's look at the first 1000 examples from the training set to make sure they're loaded in properly using our *montage* function. "
      ]
    },
    {
      "metadata": {
        "id": "IFuv4mf74d7c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "montage(X[:1000, ...])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M1HzaRJTGkmt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can also plot the distribution of the values in these 1000 training examples. "
      ]
    },
    {
      "metadata": {
        "id": "qBp35rPmGrDw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.hist(X[:1000, :].flatten(), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "unSEQDbVWbuA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ". . . and the mean and standard deviation. "
      ]
    },
    {
      "metadata": {
        "id": "0qN-CMw1WAi7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(np.mean(X))\n",
        "print(np.std(X))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C3YxBss-Wf9d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below, we normalize the data by subtracting each pixel's value by the mean value for that pixel across all of the training images and dividing by the standard deviation of that pixel across all of the images. This is a very common technique in data processing, and can help increase speed of learning and generalization of the network. "
      ]
    },
    {
      "metadata": {
        "id": "VMvRDPOG2rSE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_mean, X_std = np.mean(X, 0), np.std(X, 0)\n",
        "X = (X - X_mean) / (X_std + 1e-6)\n",
        "testX = (testX - X_mean) / (X_std + 1e-6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hTUMt4eMXJI3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's plot the normalized images now. "
      ]
    },
    {
      "metadata": {
        "id": "B1qpfJNlXIS_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "montage(X[:1000, ...])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wIHtpZCRXMtu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And we can look at the distribution of all of the values in the first 1000 images like before. "
      ]
    },
    {
      "metadata": {
        "id": "TJsvYn84HBhR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.hist(X[:1000, :].flatten(), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ToNSByL3XTn6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice how the mean is now very close to zero and the standard deviation close to one. The data is now close to a normal distribution. "
      ]
    },
    {
      "metadata": {
        "id": "NECDTXbaWIX5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(np.mean(X))\n",
        "print(np.std(X))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CAHtdCasauGY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The line below mainly serves to reset the TensorFlow graph in case you want to run the same model more than once without resetting your notebook's runtime. "
      ]
    },
    {
      "metadata": {
        "id": "DnJeP4iJjOgc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lS58PbbJXcCt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first thing we need to create for the network is the input layer. All we really need to say is what shape it should expect. For this network, we tell the network to expect any amount of examples, but they should all be 784-dimensional vectors. "
      ]
    },
    {
      "metadata": {
        "id": "M5reBX9O47YF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_layer = tflearn.input_data(shape=[None, 784])\n",
        "emb = tf.Variable(X[:5000,:], name='input_images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S57lP2aUX5u-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we have the input layer, we can create our network. The one we create here has two hidden (intermediate) layers. The first hidden layer contains 500 nodes or perceptrons, each of which will weight every pixel in the input image vector a certain amount and produce an output between -1 and +1 since we are using the *tanh* activation function. The second hidden layer has 500 nodes that all weight the responses of the 500 nodes in the first layer and produce some output between -1 and +1. Then we have a third layer also with 500 nodes that look at the second layer responses. After each of these layers, we use something called *dropout*, which is designed to increase generalization of a network by randomly removing some of the layer's responses and their weights with the given probability (here we say to keep about 70% of them). The output layer has 10 nodes, one for each of the 10 numbers (0 - 9). The goal is to get the output node corresponding to the letter in the input image to output a value of 1 and all of the other nine nodes to output 0. "
      ]
    },
    {
      "metadata": {
        "id": "7tqr3v_y5wWg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden1 = tflearn.fully_connected(input_layer, \n",
        "                                  500, \n",
        "                                  activation='tanh', \n",
        "                                  regularizer='L2', \n",
        "                                  weights_init='xavier',\n",
        "                                  name='fc1')\n",
        "hidden1 = tflearn.dropout(hidden1, 0.7)\n",
        "hidden2 = tflearn.fully_connected(hidden1, \n",
        "                                  500, \n",
        "                                  activation='tanh', \n",
        "                                  regularizer='L2',\n",
        "                                  weights_init='xavier')\n",
        "hidden2 = tflearn.dropout(hidden2, 0.7)\n",
        "hidden3 = tflearn.fully_connected(hidden2, \n",
        "                                  500, \n",
        "                                  activation='tanh', \n",
        "                                  regularizer='L2',\n",
        "                                  weights_init='xavier')\n",
        "hidden3 = tflearn.dropout(hidden3, 0.7)\n",
        "output = tflearn.fully_connected(hidden3, 10, activation='softmax')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qgb9DGwsdS0f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below, we describe what autodifferentiation optimizer we want to use to find a set of weights that can map between the input images and the respective outputs, and we specify that the learning rate should be .01. "
      ]
    },
    {
      "metadata": {
        "id": "tB-kljUX8YLx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sgd = tflearn.SGD(learning_rate=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zgZDkfZFdkWr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the line below, we describe all of the optimization parameters for the network. We are saying that the output of the network is called output, and that is what should be compared to the actual labels for each input image. Using the *categorical crossentropy* loss function, described as $H(p, q) = -\\sum p$ log$(q)$, where q is the response from the 10 output nodes and p is what the response should be. "
      ]
    },
    {
      "metadata": {
        "id": "1OHzraxQ88SK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "network = tflearn.regression(output, optimizer=sgd, \n",
        "                             loss='categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HqsUikiFbc_D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below, we call our embedding visualization function."
      ]
    },
    {
      "metadata": {
        "id": "9FVjtxW6ryBL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tensorboard_name = 'mnist_fc_tflearn'\n",
        "viz_mnist_embedding(emb, X[:5000,:], np.argmax(Y[:5000,:], 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8k19-ldpRAqu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we run the functions to start tensorboard, and a link with the address to Tensorboard will appear (you may have to run this two or three times for it to work). "
      ]
    },
    {
      "metadata": {
        "id": "f8rYuvicSGlj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "install_tensorboard_dep()\n",
        "start_tensorboard()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "io5YGdbyhMgz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, that we have told TFLearn how it should optimize the weights, we need to describe the training parameters, build the network, and execute the training.  "
      ]
    },
    {
      "metadata": {
        "id": "JnKVzs3m6Jm1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = tflearn.DNN(network, tensorboard_verbose=0)\n",
        "\n",
        "model.fit(X, Y, n_epoch=10,\n",
        "          validation_set=(testX, testY),\n",
        "          batch_size=100,\n",
        "          snapshot_step=200,\n",
        "          show_metric=True,\n",
        "          run_id=tensorboard_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u4YUQZ4fhysc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If our model reached a good loss and / or accuracy, we might want to save the network for later training or for implementation into an application. The line below describes how to do this. You can rename it to whatever you want by changing the red text. "
      ]
    },
    {
      "metadata": {
        "id": "YkezzQoaCOoM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save('fully_connected_mnist')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N5XMOJPMiFy9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below, we show how to read in the model, but not for further training. Here, we will explore the first layer of weights in the network to see what each of the 500 nodes in the first hidden layer was looking for in the input images. "
      ]
    },
    {
      "metadata": {
        "id": "lj4VBeUxCZTO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python import pywrap_tensorflow\n",
        "reader = pywrap_tensorflow.NewCheckpointReader('fully_connected_mnist')\n",
        "vars = reader.get_variable_to_shape_map()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zBR87p0Mz1rM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can print all of the pieces of the model, including the metrics, objective function, etc."
      ]
    },
    {
      "metadata": {
        "id": "tR9ukIkjzyGc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lyYxI1dU0AyV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first layer of weights is called **FullyConnected/W**, so we can load that layer in. "
      ]
    },
    {
      "metadata": {
        "id": "IQgvHm8zz7v0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden1_w = reader.get_tensor('FullyConnected/W')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N12_KT-qyCY6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can print the shape of it now that we have loaded it as a numpy array. It should be 784 x 500 because it is the weight layer between the input layer, which has 784 nodes, and the first hidden layer, which has 500 nodes."
      ]
    },
    {
      "metadata": {
        "id": "oSyzwQI-EBH5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(hidden1_w.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GU9dh1FsybCU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We want to see what each of the 500 nodes in the first hidden layer are looking for in the input layer, so if we want to use our montage function to visualize this, we need to transpose the matrix so each feature is a row in the matrix."
      ]
    },
    {
      "metadata": {
        "id": "OpFmL8gS0Lld",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden1_w = hidden1_w.transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GFS47CiEEyk1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(hidden1_w.shape)\n",
        "montage(hidden1_w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zE7TnjTluYWg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Try it Yourself:\n",
        "Below, try to extract and view the weights from all of the subsequent layers. "
      ]
    },
    {
      "metadata": {
        "id": "s4e3YqSEujpo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}